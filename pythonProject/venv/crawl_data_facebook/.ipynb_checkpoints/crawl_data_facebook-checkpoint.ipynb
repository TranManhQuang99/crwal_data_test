{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from selenium.webdriver.chrome import options\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from  selenium.webdriver.common.by import By\n",
    "import csv\n",
    "import json\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--disable-notifications')\n",
    "s = Service(\"C:/Users/armi1/PycharmProjects/pythonProject/venv/crawl_data_facebook\\chromedriver.exe\")\n",
    "browser = webdriver.Chrome(service=s,options=chrome_options)\n",
    "# browser.implicitly_wait(10)\n",
    "\n",
    "browser.get(\"https://vi-vn.facebook.com/\")\n",
    "cockies = pickle.load(open(r\"C:\\Users\\armi1\\PycharmProjects\\pythonProject\\venv\\crawl_data_facebook\\my_cockie.pkl\",\"rb\"))\n",
    "for i in cockies:\n",
    "    browser.add_cookie(i)\n",
    "browser.get(\"https://vi-vn.facebook.com/\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_word = 'data analyst'\n",
    "def search_keyword(key_word):\n",
    "    search = browser.find_element(By.CLASS_NAME,'a5nuqjux')\n",
    "    search.send_keys(key_word)\n",
    "    search.send_keys(Keys.RETURN)\n",
    "    sleep(2)\n",
    "    browser.get(f\"https://www.facebook.com/search/groups?q={key_word}\")\n",
    "\n",
    "search_keyword(key_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_group(number_group):\n",
    "    all_url_group = []\n",
    "    for i in range(number_group):\n",
    "        browser.execute_script(f'window.scrollTo(0,{i + 1}*1080)')\n",
    "        sleep(1)\n",
    "        page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        url = page_source.find_all('span',class_='a8c37x1j ni8dbmo4 stjgntxs l9j0dhe7 nkwizq5d roh60bw9 hop8lmos scwd0bx6 n8tt0mok hyh9befq jwdofwj8 r8blr3vg')\n",
    "\n",
    "        for i in url:\n",
    "            el = i.find(href=True)\n",
    "            link_url= el['href']\n",
    "            if link_url not in all_url_group:\n",
    "                all_url_group.append(link_url)\n",
    "    return all_url_group\n",
    "\n",
    "all_url_group = get_url_group(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_group():\n",
    "    for link in all_url_group:\n",
    "        sleep(10)\n",
    "        browser.get(link)\n",
    "        sleep(5)\n",
    "        try:\n",
    "            join_group = browser.find_element(By.CSS_SELECTOR,'.rq0escxv.l9j0dhe7.du4w35lb.j83agx80.cbu4d94t.pfnyh3mw.d2edcug0.ri2l8tne.ph5uu5jm.b3onmgus.e5nlhep0.ecm0bbzt.gloz99to.r516eku6.k83vx86k').click()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "join_group()\n",
    "\n",
    "sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.facebook.com/groups/568482360024983/\n",
      "https://www.facebook.com/groups/datanalyticsvn/\n",
      "https://www.facebook.com/groups/dataanalyticsvn/\n",
      "https://www.facebook.com/groups/512959919288454/\n",
      "https://www.facebook.com/groups/421314039291688/\n",
      "https://www.facebook.com/groups/da100vn/\n",
      "https://www.facebook.com/groups/1669228373166406/\n",
      "https://www.facebook.com/groups/nghephantichdulieu/\n",
      "https://www.facebook.com/groups/302182307947490/\n",
      "https://www.facebook.com/groups/160325109320298/\n",
      "https://www.facebook.com/groups/1564312220624643/\n",
      "https://www.facebook.com/groups/DataAnalyticsVietnam/\n",
      "https://www.facebook.com/groups/427259511058712/\n",
      "https://www.facebook.com/groups/datapot.vn/\n",
      "https://www.facebook.com/groups/talent5group/\n",
      "https://www.facebook.com/groups/nghedata/\n",
      "https://www.facebook.com/groups/allbig/\n",
      "https://www.facebook.com/groups/830880067080683/\n",
      "https://www.facebook.com/groups/vietnam.data.analysis.professional/\n",
      "https://www.facebook.com/groups/870665749718859/\n",
      "https://www.facebook.com/groups/masteringdataanalytics/\n",
      "https://www.facebook.com/groups/583821108695586/\n",
      "https://www.facebook.com/groups/datadatascienceaimldl/\n",
      "https://www.facebook.com/groups/vnbdc/\n",
      "https://www.facebook.com/groups/Bigdata.DataEngineer.PythonDeveloper.Vietnam/\n",
      "https://www.facebook.com/groups/579613165505838/\n",
      "https://www.facebook.com/groups/datascience.bigdata.businessIntelligence.vietnam/\n",
      "https://www.facebook.com/groups/bigdatastatistics/\n",
      "https://www.facebook.com/groups/1276179282431647/\n",
      "https://www.facebook.com/groups/dataanalysisandprocessing/\n",
      "https://www.facebook.com/groups/943616629163360/\n",
      "https://www.facebook.com/groups/380882469110853/\n",
      "https://www.facebook.com/groups/DataScience.MachineLearning.ArtificialIntellegence/\n",
      "https://www.facebook.com/groups/3455675624547627/\n",
      "https://www.facebook.com/groups/354988484956074/\n",
      "https://www.facebook.com/groups/machinelearningforum/\n",
      "https://www.facebook.com/groups/BigDataPakistan/\n",
      "https://www.facebook.com/groups/995474220466742/\n",
      "https://www.facebook.com/groups/BizAnalyticsVN/\n",
      "https://www.facebook.com/groups/data.analytics/\n",
      "https://www.facebook.com/groups/997975157020313/\n",
      "https://www.facebook.com/groups/Data.Driven.Marketing.Communication/\n",
      "https://www.facebook.com/groups/Data.Driven.Sale.Strategy/\n",
      "https://www.facebook.com/groups/theinsaneapp/\n",
      "https://www.facebook.com/groups/researchlabbd/\n",
      "https://www.facebook.com/groups/VietSTAT/\n",
      "https://www.facebook.com/groups/aimlpljobs/\n",
      "https://www.facebook.com/groups/BigDataStartUp/\n",
      "https://www.facebook.com/groups/2583515898581823/\n",
      "https://www.facebook.com/groups/251502155764625/\n"
     ]
    }
   ],
   "source": [
    "def check_join_group():\n",
    "    group_join = []\n",
    "    for link in all_url_group:\n",
    "        browser.get(link)\n",
    "        sleep(15)\n",
    "        page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        try:\n",
    "            join_group = page_source.find('span',class_='d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j fe6kdd0r mau55g9w c8b282yb keod5gw0 nxhoafnm aigsh9s9 d3f4x2em iv3no6db jq4qci2q a3bd9o3v lrazzd5p a57itxjd').get_text()\n",
    "\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "        if join_group == 'Đã tham gia':\n",
    "            group_join.append(link)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        print(link)\n",
    "    return group_join\n",
    "\n",
    "group_join = check_join_group()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_url():\n",
    "    url_mbasic = []\n",
    "    for i in group_join:\n",
    "        replace_facebook = i.replace(\"www\", \"mbasic\")\n",
    "        url_mbasic.append(replace_facebook)\n",
    "\n",
    "    return url_mbasic\n",
    "url_mbasic = convert_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_url():\n",
    "    find_id_x = []\n",
    "    page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    find_id = page_source.find_all('a', class_='ed')\n",
    "    for i in find_id:\n",
    "        url = i.get('href')\n",
    "        find_id_x.append(url)\n",
    "    find_id_x = list(dict.fromkeys(find_id_x))\n",
    "    return find_id_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_url_post(number_post):\n",
    "    url_all_post = []\n",
    "    for url_group in url_mbasic:\n",
    "        sleep(9)\n",
    "        browser.get(url_group)\n",
    "        sleep(5)\n",
    "        for number in range(number_post):\n",
    "            page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "            find_id = get_url()\n",
    "            url_all_post = url_all_post + find_id\n",
    "            sleep(10)\n",
    "            try:\n",
    "                x = browser.find_element(By.XPATH, '//*[@id=\"m_group_stories_container\"]/div').click()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return url_all_post\n",
    "\n",
    "url_all_post = get_url_post(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_link():\n",
    "    url = []\n",
    "    link = list(dict.fromkeys(url_all_post))\n",
    "    x = 'https://mbasic.facebook.com'\n",
    "    for i in link:\n",
    "        if x in i:\n",
    "            url.append(i)\n",
    "    return url\n",
    "url = filter_link()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_url_link_post():\n",
    "    url_post = []\n",
    "    for i in url:\n",
    "        replace_post_facebook = i.replace(\"mbasic\", \"www\")\n",
    "        url_post.append(replace_post_facebook)\n",
    "    return url_post\n",
    "url_post = convert_url_link_post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Social_Network': 'Facebook', 'Id': '421314039291688641219440634479', 'Key_word': 'data analyst', 'Names': '小桃 đã chia sẻ một bài viết.', 'Link_post': 'https://www.facebook.com/groups/421314039291688/permalink/641219440634479/?refid=18&_ft_=qid.-7063715350312950417%3Amf_story_key.641219440634479%3Atop_level_post_id.641219440634479%3Atl_objid.641219440634479%3Acontent_owner_id_new.100021474320580%3Aoriginal_content_id.140623035061004%3Aoriginal_content_owner_id.110240674765907%3Apage_id.110240674765907%3Asrc.22%3Astory_location.6%3Aattached_story_attachment_style.album%3Afilter.GroupStoriesByActivityEntQuery%3Aott.AX8Pt9B1Pg-oAVqu%3Atds_flgs.3%3Apage_insights.%7B%22110240674765907%22%3A%7B%22page_id%22%3A110240674765907%2C%22page_id_type%22%3A%22page%22%2C%22actor_id%22%3A100021474320580%2C%22attached_story%22%3A%7B%22page_id%22%3A110240674765907%2C%22page_id_type%22%3A%22page%22%2C%22actor_id%22%3A110240674765907%2C%22dm%22%3A%7B%22isShare%22%3A0%2C%22originalPostOwnerID%22%3A0%7D%2C%22psn%22%3A%22EntStatusCreationStory%22%2C%22post_context%22%3A%7B%22object_fbtype%22%3A266%2C%22publish_time%22%3A1641643381%2C%22story_name%22%3A%22EntStatusCreationStory%22%2C%22story_fbid%22%3A%5B140623035061004%5D%7D%2C%22role%22%3A1%2C%22sl%22%3A6%7D%2C%22dm%22%3A%7B%22isShare%22%3A0%2C%22originalPostOwnerID%22%3A0%7D%2C%22psn%22%3A%22EntGroupMallPostCreationStory%22%2C%22role%22%3A1%2C%22sl%22%3A6%2C%22targets%22%3A%5B%7B%22actor_id%22%3A100021474320580%2C%22page_id%22%3A110240674765907%2C%22post_id%22%3A140623035061004%2C%22role%22%3A1%2C%22share_id%22%3A0%7D%5D%7D%2C%22421314039291688%22%3A%7B%22page_id%22%3A421314039291688%2C%22page_id_type%22%3A%22group%22%2C%22actor_id%22%3A100021474320580%2C%22dm%22%3A%7B%22isShare%22%3A1%2C%22originalPostOwnerID%22%3A140623035061004%7D%2C%22psn%22%3A%22EntGroupMallPostCreationStory%22%2C%22post_context%22%3A%7B%22object_fbtype%22%3A657%2C%22publish_time%22%3A1641643802%2C%22story_name%22%3A%22EntGroupMallPostCreationStory%22%2C%22story_fbid%22%3A%5B641219440634479%5D%7D%2C%22role%22%3A1%2C%22sl%22%3A6%7D%7D&__tn__=%2AW-R', 'post': 'Đây là bài viết thứ hai của nhóm chúng em. Lần này bọn em xin chia sẻ một số trải nghiệm của bản thân về roadmap và 1 số nguồn roadmap để tham khảo. Rất mong nhận được sự ủng hộ của mọi người', 'comment': [], 'device': None, 'location': None, 'Job_title': None, 'time': '8/1/2022', 'user': 'None'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-21-2d8d2beaeaa6>\", line 161, in <module>\n",
      "    get_data_facebook()\n",
      "  File \"<ipython-input-21-2d8d2beaeaa6>\", line 5, in get_data_facebook\n",
      "    sleep(10)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 744, in getmodule\n",
      "    for modname, module in sys.modules.copy().items():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-2d8d2beaeaa6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m \u001b[0mget_data_facebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-2d8d2beaeaa6>\u001b[0m in \u001b[0;36mget_data_facebook\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2060\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2063\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "def get_data_facebook():\n",
    "    for url in url_post:\n",
    "        sleep(2)\n",
    "        browser.get(url)\n",
    "        sleep(10)\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                browser.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div/div[3]/div/div/div[1]/div[1]/div[4]/div/div/div/div/div/div/div/div[1]/div/div/div/div/div/div/div/div/div/div/div[2]/div/div[4]/div/div/div[2]/div[2]/div[1]\").click()\n",
    "            except:\n",
    "                browser.find_element(By.XPATH,\n",
    "                                 '/html/body/div[1]/div/div[1]/div/div[3]/div/div/div[1]/div[1]/div[4]/div/div/div/div/div/div/div[1]/div/div/div/div/div/div/div/div/div/div/div[2]/div/div[4]/div/div/div[2]/div[2]/div[1]/div[2]/span').click()\n",
    "        except:\n",
    "            click_deteth = browser.find_element(By.XPATH, \"//html\").click()\n",
    "\n",
    "        sleep(1)\n",
    "        page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "        # ten nguoi post bai\n",
    "        try:\n",
    "            name_post = page_source.find('h2',\n",
    "                                     class_='gmql0nx0 l94mrbxd p1ri9a11 lzcic4wl aahdfvyu hzawbc8m').get_text()\n",
    "            name_post = name_post.replace(\"đã chia sẻ một liên kết.\",\"\").strip()\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # noi dung bai post\n",
    "        try:\n",
    "            try:\n",
    "                try:\n",
    "                    try:\n",
    "                        try:\n",
    "                            post = page_source.find('div',class_='ecm0bbzt hv4rvrfc dati1w0a e5nlhep0').get_text()\n",
    "                        except:\n",
    "                            post = page_source.find('div',class_='kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x c1et5uql ii04i59q').get_text()\n",
    "                    except:\n",
    "                        xpath_post = browser.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div/div[3]/div/div/div[1]/div[1]/div[4]/div/div/div/div/div/div/div[1]/div/div/div/div/div/div/div/div/div/div/div[2]/div/div[3]/div')\n",
    "                        post = xpath_post.text\n",
    "                except:\n",
    "                    post = page_source.find('div',class_='dati1w0a ihqw7lf3 hv4rvrfc ecm0bbzt').get_text()\n",
    "            except:\n",
    "                post = page_source.find('div',class_='kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x c1et5uql').get_text()\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # link nguoi post bai\n",
    "        try:\n",
    "            href = page_source.find('a',\n",
    "                                class_='oajrlxb2 gs1a9yip g5ia77u1 mtkw9kbi tlpljxtp qensuy8j ppp5ayq2 goun2846 ccm00jje s44p3ltw mk2mc5f4 rt8b4zig n8ej3o3l agehan2d sk4xxmp2 rq0escxv nhd2j8a9 mg4g778l pfnyh3mw p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x tgvbjcpo hpfvmrgz jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso l9j0dhe7 i1ao9s8h esuyzwwr f1sip0of du4w35lb n00je7tq arfg74bv qs9ysxi8 k77z8yql btwxx1t3 abiwlrkh p8dawk7l lzcic4wl oo9gr5id q9uorilb').get('href')\n",
    "        except: \n",
    "            continue\n",
    "            \n",
    "        # ID nguoi viet bai\n",
    "        try:\n",
    "            ID = re.findall('\\d+', href)[1]\n",
    "        except: \n",
    "            continue\n",
    "            \n",
    "        # Link fb nguoi post bai\n",
    "        link_persion_post = 'https://www.facebook.com/' + ID\n",
    "\n",
    "        # Lay ten nguoi comment\n",
    "        page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "        all_comment = page_source.find('div', class_='cwj9ozl2 tvmbv18p')\n",
    "        extract_cmt = all_comment.find_all('li')\n",
    "        name_comment = []\n",
    "        comment = []\n",
    "        Link_persion_comment = []\n",
    "        all_commment = []\n",
    "        for li in extract_cmt:\n",
    "            try:\n",
    "                all_comment = li.find('span', class_='d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j fe6kdd0r mau55g9w c8b282yb keod5gw0 nxhoafnm aigsh9s9 d9wwppkn mdeji52x e9vueds3 j5wam9gi lrazzd5p oo9gr5id').get_text().strip()\n",
    "                name_comment.append(all_comment)\n",
    "            except:\n",
    "                continue\n",
    "            try:\n",
    "                span_comment = li.find('div', class_='kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x c1et5uql').get_text()\n",
    "                comment.append(span_comment)\n",
    "            except:\n",
    "                continue\n",
    "            try:\n",
    "                url_persion_comment = li.find('a',\n",
    "                                          class_='oajrlxb2 g5ia77u1 qu0x051f esr5mh6w e9989ue4 r7d6kgcz rq0escxv nhd2j8a9 nc684nl6 p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso i1ao9s8h esuyzwwr f1sip0of lzcic4wl gmql0nx0 gpro0wi8').get('href')\n",
    "                ID_persion_comment = 'https://www.facebook.com/' + re.findall('\\d+', url_persion_comment)[1]\n",
    "                Link_persion_comment.append(ID_persion_comment)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        map_all_comment = map(lambda x, y, z: x + str(':') + y + str(':') + z, name_comment, Link_persion_comment,comment)\n",
    "        for i in map_all_comment:\n",
    "            all_commment.append(i)\n",
    "\n",
    "        # ID_post\n",
    "        ID_POST = re.findall('\\d+', url)[0] + re.findall('\\d+', url)[1]\n",
    "\n",
    "        # # Time post\n",
    "        try:\n",
    "            try:\n",
    "                try:\n",
    "                    time_post_text = page_source.find('b',class_='t5a262vz nc684nl6 ihxqhq3m l94mrbxd aenfhxwr l9j0dhe7 sdhka5h4').get_text()\n",
    "\n",
    "                except:\n",
    "                    time_post_text = page_source.find('b',class_='t5a262vz aenfhxwr b6zbclly l9j0dhe7 sdhka5h4').get_text()\n",
    "            except:\n",
    "                time_post_text = browser.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div/div[3]/div/div/div[1]/div[1]/div[4]/div/div/div/div/div/div/div[1]/div/div/div/div/div/div/div/div/div/div/div[2]/div/div[2]/div/div[2]/div/div[2]/span/span/span[2]/span').text\n",
    "        except:\n",
    "            time_post_text = page_source.find('span',class_='j1lvzwm4 stjgntxs ni8dbmo4 q9uorilb gpro0wi8').get_text()\n",
    "        \n",
    "        \n",
    "        RE_URL = re.findall('\\d+', time_post_text)\n",
    "        \n",
    "        try:\n",
    "            if len(RE_URL) == 2:\n",
    "                Number = re.findall('\\d+', str(datetime.datetime.now()))\n",
    "                time_post = Number[2]+str('/')+Number[1]+str('/')+ Number[0]\n",
    "            if len(RE_URL) == 5:\n",
    "                Number = re.findall('\\d+', time_post_text)\n",
    "                time_post = RE_URL[0] + str('/') + RE_URL[1] + '/2021'  \n",
    "            if len(RE_URL) == 4:\n",
    "                Number = re.findall('\\d+', time_post_text)\n",
    "                time_post = RE_URL[0] + str('/') + RE_URL[1] + '/2022' \n",
    "        except:\n",
    "            Number = re.findall('\\d+', str(datetime.datetime.now()))\n",
    "            time_post = Number[2]+str('/')+Number[1]+str('/')+ Number[0]\n",
    "\n",
    "        my_details = {\n",
    "            'Social_Network': 'Facebook',\n",
    "            'Id': ID_POST,\n",
    "            'Key_word': key_word,\n",
    "            'Names': name_post,\n",
    "            'Link_post': url,\n",
    "            'post': post,\n",
    "            'comment': all_commment,\n",
    "            'device': None,\n",
    "            'location': None,\n",
    "            'Job_title': None,\n",
    "            'time': time_post,\n",
    "            'user': \"None\"\n",
    "        }\n",
    "        print(my_details)\n",
    "\n",
    "\n",
    "\n",
    "        cluster = MongoClient(\n",
    "            \"mongodb+srv://minh15599:123456asdf@cluster0.wkj8v.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "        db = cluster['123a']\n",
    "        collection = db['myapp_employee']\n",
    "\n",
    "        # check trung bai viet\n",
    "\n",
    "        myquery = {\"Id\": ID_POST}\n",
    "        check_duplicate = []\n",
    "        mydoc = collection.find(myquery)\n",
    "        for i in mydoc:\n",
    "            check_duplicate.append(i)\n",
    "        if check_duplicate == []:\n",
    "            collection.insert_one(my_details)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "get_data_facebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conda install jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trương Tiến Toàn:https://www.facebook.com/100006143347231:Trước mình dùng Google cloud và Azure thì thấy khá ổn, AWS ngon nhất nhưng đắt nhất ', 'Tien Anh Vu:https://www.facebook.com/100009794611068:Trương Tiến Toàn anh dùng VM của GCP ấy ạ, em thấy thuê cả tháng toàn vài nghìn ấy, kể cả V100 với low ram', 'Phát Trần:https://www.facebook.com/100010870410803:Colab pro thì T4, Pro+ nghe bảo ra P100 suốt', 'Ju Dan:https://www.facebook.com/100011319374209:Mình dùng colab pro và toàn dc p100. Bạn mở được bao nhiêu session cùng lúc vậy, thời gian giới hạn là bao nhiêu', 'Tien Anh Vu:https://www.facebook.com/100009794611068:Ju Dan mình mở đc tầm max 3 với chế độ standard ram, tầm 2 cái high ram, khoảng thời gian thì tùy, bạn có chọn background execution không', 'Ju Dan:https://www.facebook.com/100011319374209:Tien Anh Vu mình cũng tương tự nhưng chỉ dc 1 cái high ram (25g)', 'Nguyen Nguyen:https://www.facebook.com/100048304336999:https://jarvislabs.ai/ Bạn tham khảo thử nhé!', 'Hoàng Gia Huy:https://www.facebook.com/100006040810652:Tuỳ vào nhân phẩm nữa bác, mình xài pro thường lúc train nhẹ cho vui thì toàn ra p100 lúc up full data tự nhiên toàn t4 =))', 'Danh Bai Thi:https://www.facebook.com/100013029124659:bạn dùng thử https://aistudio.baidu.com/ hỗ trợ miễn V100 48h/tuần. Hạn chế là không hỗ trợ tensorflow + pytorch, bạn phải làm quen với thư viện paddlepaddle của Baidu.']\n"
     ]
    }
   ],
   "source": [
    "page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "all_comment = page_source.find('div', class_='cwj9ozl2 tvmbv18p')\n",
    "extract_cmt = all_comment.find_all('li')\n",
    "name_comment = []\n",
    "comment = []\n",
    "Link_persion_comment = []\n",
    "all_commment = []\n",
    "for li in extract_cmt:\n",
    "    try:\n",
    "        all_comment = li.find('span', class_='d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j fe6kdd0r mau55g9w c8b282yb keod5gw0 nxhoafnm aigsh9s9 d9wwppkn mdeji52x e9vueds3 j5wam9gi lrazzd5p oo9gr5id').get_text().strip()\n",
    "        name_comment.append(all_comment)\n",
    "    except:\n",
    "        continue\n",
    "    try:\n",
    "        span_comment = li.find('div', class_='kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x c1et5uql').get_text()\n",
    "        comment.append(span_comment)\n",
    "    except:\n",
    "        continue\n",
    "    try:\n",
    "        url_persion_comment = li.find('a',\n",
    "                                  class_='oajrlxb2 g5ia77u1 qu0x051f esr5mh6w e9989ue4 r7d6kgcz rq0escxv nhd2j8a9 nc684nl6 p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso i1ao9s8h esuyzwwr f1sip0of lzcic4wl gmql0nx0 gpro0wi8').get('href')\n",
    "        ID_persion_comment = 'https://www.facebook.com/' + re.findall('\\d+', url_persion_comment)[1]\n",
    "        Link_persion_comment.append(ID_persion_comment)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "map_all_comment = map(lambda x, y, z: x + str(':') + y + str(':') + z, name_comment, Link_persion_comment,comment)\n",
    "for i in map_all_comment:\n",
    "    all_commment.append(i)\n",
    "\n",
    "    \n",
    "print(all_commment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trương Tiến Toàn', 'Tien Anh Vu', 'Phát Trần', 'Ju Dan', 'Tien Anh Vu', 'Ju Dan', 'Nguyen Nguyen', 'Hoàng Gia Huy', 'Danh Bai Thi']\n"
     ]
    }
   ],
   "source": [
    "def get_url_post(number_post):\n",
    "    url_all_post = []\n",
    "    for url_group in url_mbasic:\n",
    "        sleep(9)\n",
    "        browser.get(url_group)\n",
    "        sleep(5)\n",
    "        for number in range(number_post):\n",
    "            page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "            find_id = get_url()\n",
    "            url_all_post = url_all_post + find_id\n",
    "            sleep(2)\n",
    "            try:\n",
    "                x = browser.find_element(By.XPATH, '//*[@id=\"m_group_stories_container\"]/div').click()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return url_all_post\n",
    "\n",
    "url_all_post = get_url_post(10)\n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(url_post))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url():\n",
    "    find_id_x = []\n",
    "    page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    find_id = page_source.find_all('a', class_='ed')\n",
    "    for i in find_id:\n",
    "        url = i.get('href')\n",
    "        find_id_x.append(url)\n",
    "    find_id_x = list(dict.fromkeys(find_id_x))\n",
    "    return find_id_x\n",
    "\n",
    "get_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677\n"
     ]
    }
   ],
   "source": [
    "print(len(url_all_post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/1/2022\n"
     ]
    }
   ],
   "source": [
    "page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "try:\n",
    "    try:\n",
    "        try:\n",
    "            time_post_text = page_source.find('b',class_='t5a262vz nc684nl6 ihxqhq3m l94mrbxd aenfhxwr l9j0dhe7 sdhka5h4').get_text()\n",
    "\n",
    "        except:\n",
    "            time_post_text = page_source.find('b',class_='t5a262vz aenfhxwr b6zbclly l9j0dhe7 sdhka5h4').get_text()\n",
    "    except:\n",
    "        time_post_text = browser.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div/div[3]/div/div/div[1]/div[1]/div[4]/div/div/div/div/div/div/div[1]/div/div/div/div/div/div/div/div/div/div/div[2]/div/div[2]/div/div[2]/div/div[2]/span/span/span[2]/span').text\n",
    "except:\n",
    "    time_post_text = page_source.find('span',class_='j1lvzwm4 stjgntxs ni8dbmo4 q9uorilb gpro0wi8').get_text()\n",
    "\n",
    "\n",
    "RE_URL = re.findall('\\d+', time_post_text)\n",
    "    \n",
    "try:\n",
    "    if len(RE_URL) == 2:\n",
    "        Number = re.findall('\\d+', str(datetime.datetime.now()))\n",
    "        time_post = Number[2]+str('/')+Number[1]+str('/')+ Number[0]\n",
    "    if len(RE_URL) == 5:\n",
    "        Number = re.findall('\\d+', time_post_text)\n",
    "        time_post = RE_URL[0] + str('/') + RE_URL[1] + '/2021'  \n",
    "    if len(RE_URL) == 4:\n",
    "        Number = re.findall('\\d+', time_post_text)\n",
    "        time_post = RE_URL[0] + str('/') + RE_URL[1] + '/2022'\n",
    "except:\n",
    "    Number = re.findall('\\d+', str(datetime.datetime.now()))\n",
    "    time_post = Number[2]+str('/')+Number[1]+str('/')+ Number[0]\n",
    "    \n",
    "print(time_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://mbasic.facebook.com/groups/568482360024983/', 'https://mbasic.facebook.com/groups/datanalyticsvn/', 'https://mbasic.facebook.com/groups/nghephantichdulieu/', 'https://mbasic.facebook.com/groups/dataanalyticsvn/', 'https://mbasic.facebook.com/groups/512959919288454/', 'https://mbasic.facebook.com/groups/1669228373166406/', 'https://mbasic.facebook.com/groups/da100vn/', 'https://mbasic.facebook.com/groups/421314039291688/', 'https://mbasic.facebook.com/groups/302182307947490/', 'https://mbasic.facebook.com/groups/1564312220624643/', 'https://mbasic.facebook.com/groups/160325109320298/', 'https://mbasic.facebook.com/groups/DataAnalyticsVietnam/', 'https://mbasic.facebook.com/groups/427259511058712/', 'https://mbasic.facebook.com/groups/datapot.vn/', 'https://mbasic.facebook.com/groups/talent5group/', 'https://mbasic.facebook.com/groups/nghedata/', 'https://mbasic.facebook.com/groups/583821108695586/', 'https://mbasic.facebook.com/groups/870665749718859/', 'https://mbasic.facebook.com/groups/allbig/', 'https://mbasic.facebook.com/groups/830880067080683/', 'https://mbasic.facebook.com/groups/vietnam.data.analysis.professional/', 'https://mbasic.facebook.com/groups/masteringdataanalytics/', 'https://mbasic.facebook.com/groups/datascience.bigdata.businessIntelligence.vietnam/', 'https://mbasic.facebook.com/groups/579613165505838/', 'https://mbasic.facebook.com/groups/1276179282431647/', 'https://mbasic.facebook.com/groups/datadatascienceaimldl/', 'https://mbasic.facebook.com/groups/Bigdata.DataEngineer.PythonDeveloper.Vietnam/', 'https://mbasic.facebook.com/groups/bigdatastatistics/', 'https://mbasic.facebook.com/groups/995474220466742/', 'https://mbasic.facebook.com/groups/vnbdc/', 'https://mbasic.facebook.com/groups/machinelearningforum/', 'https://mbasic.facebook.com/groups/BigDataPakistan/', 'https://mbasic.facebook.com/groups/DataScience.MachineLearning.ArtificialIntellegence/', 'https://mbasic.facebook.com/groups/3455675624547627/', 'https://mbasic.facebook.com/groups/380882469110853/', 'https://mbasic.facebook.com/groups/943616629163360/', 'https://mbasic.facebook.com/groups/data.analytics/', 'https://mbasic.facebook.com/groups/dataanalysisandprocessing/', 'https://mbasic.facebook.com/groups/354988484956074/', 'https://mbasic.facebook.com/groups/BizAnalyticsVN/', 'https://mbasic.facebook.com/groups/997975157020313/', 'https://mbasic.facebook.com/groups/Data.Driven.Marketing.Communication/', 'https://mbasic.facebook.com/groups/VietSTAT/', 'https://mbasic.facebook.com/groups/theinsaneapp/', 'https://mbasic.facebook.com/groups/researchlabbd/', 'https://mbasic.facebook.com/groups/251502155764625/', 'https://mbasic.facebook.com/groups/Data.Driven.Sale.Strategy/', 'https://mbasic.facebook.com/groups/aimlpljobs/', 'https://mbasic.facebook.com/groups/2583515898581823/', 'https://mbasic.facebook.com/groups/1358677514334626/', 'https://mbasic.facebook.com/groups/mathforaibigdata/', 'https://mbasic.facebook.com/groups/BigDataStartUp/', 'https://mbasic.facebook.com/groups/GreatMachineLearningcomminity/', 'https://mbasic.facebook.com/groups/mldeeplearning/', 'https://mbasic.facebook.com/groups/1939366696159868/', 'https://mbasic.facebook.com/groups/upworkdeveloper/']\n"
     ]
    }
   ],
   "source": [
    "print(url_mbasic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get('https://mbasic.facebook.com/groups/568482360024983/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cầu Giấy-HN] VIETTEL tuyển BA KHÔNG YÊU CẦU KN. Thu nhập 10 - 25tr/tháng. IB mình gửi JD nhé\n"
     ]
    }
   ],
   "source": [
    "page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "post = page_source.find('div',class_='kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x c1et5uql').get_text()\n",
    "print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
